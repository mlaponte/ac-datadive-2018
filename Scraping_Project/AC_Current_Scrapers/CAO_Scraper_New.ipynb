{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T21:13:44.989710Z",
     "start_time": "2018-06-23T21:13:44.961120Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_doc_links(page_soup):\n",
    "    doc_links=[]\n",
    "    doc = page_soup.find('p',{'id':'ctl00_MainContent_documentLink'}).find('a').get('href')\n",
    "    doc_link = \"http://www.cao-ombudsman.org/cases/\" + doc.split(\"'\")[1]\n",
    "    doc_links.append(doc_link)\n",
    "    return doc_links\n",
    "def parse_pdf_links(doc_links):\n",
    "    for url in doc_links:\n",
    "        res = requests.get(url)\n",
    "        if res.status_code != 200:\n",
    "            logger.info(f\"{url} is broken\")\n",
    "            continue    \n",
    "        #get doc page data\n",
    "        try:\n",
    "            doc_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        except: \n",
    "            doc_soup = \"\"\n",
    "            continue\n",
    "        try:\n",
    "            pdf_links = doc_soup.findAll('a')\n",
    "            pdf_links_list = ['http://www.cao-ombudsman.org/cases/document-links/' + link.get('href') for link in pdf_links[1:]]\n",
    "            pdf_links_comma_seperated = \", \".join(pdf_links_list)\n",
    "        except:\n",
    "            pdf_links_comma_seperated = \"\"\n",
    "    return pdf_links_comma_seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T21:40:38.779113Z",
     "start_time": "2018-06-23T21:40:34.472122Z"
    },
    "code_folding": [
     0,
     16,
     29,
     37,
     45,
     53,
     61,
     69,
     77,
     85,
     93,
     111,
     119,
     160,
     172,
     185,
     194,
     203,
     214,
     223,
     232,
     241,
     252,
     261
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import unicodecsv\n",
    "import requests\n",
    "import logging\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from datamodel import Fields\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.remote.remote_connection import LOGGER\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from scraperutils import write_csv\n",
    "\n",
    "# Disable selenium DEBUG level logging\n",
    "LOGGER.setLevel(logging.WARNING)\n",
    "\n",
    "# Disable requests and urllib3 DEBUG level logging\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "\n",
    "# Create logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_doc_links(page_soup):\n",
    "    doc_links=[]\n",
    "    doc = page_soup.find('p',{'id':'ctl00_MainContent_documentLink'}).find('a').get('href')\n",
    "    doc_link = \"http://www.cao-ombudsman.org/cases/\" + doc.split(\"'\")[1]\n",
    "    doc_links.append(doc_link)\n",
    "    return doc_links\n",
    "def parse_pdf_links(doc_links):\n",
    "    for url in doc_links:\n",
    "        res = requests.get(url)\n",
    "        if res.status_code != 200:\n",
    "            logger.info(\"%s is broken\".format(url))\n",
    "            continue    \n",
    "        #get doc page data\n",
    "        try:\n",
    "            doc_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        except: \n",
    "            doc_soup = \"\"\n",
    "            continue\n",
    "        try:\n",
    "            pdf_links = doc_soup.findAll('a')\n",
    "            pdf_links_list = ['http://www.cao-ombudsman.org/cases/document-links/' + link.get('href') for link in pdf_links[1:]]\n",
    "            pdf_links_comma_seperated = \", \".join(pdf_links_list)\n",
    "        except:\n",
    "            pdf_links_comma_seperated = \"\"\n",
    "    return pdf_links_comma_seperated\n",
    "\n",
    "def get_page_source(url):\n",
    "    logger.info(\"Starting chrome driver\")\n",
    "    driver = webdriver.Chrome()\n",
    "    logger.info(\"Going to {link}\".format(link = url))\n",
    "    driver.get(url)\n",
    "    logger.info(\"Waiting for page to load\")\n",
    "    time.sleep(3)\n",
    "    logger.info(\"Clicking the search button\")\n",
    "    button = driver.find_element_by_id('ctl00_MainContent_btnSearch').click()\n",
    "    time.sleep(2)\n",
    "    logger.info(\"Getting the page source\")\n",
    "    s = driver.page_source\n",
    "    logger.info(\"Closing the driver\")\n",
    "    driver.close()\n",
    "    return s\n",
    "\n",
    "def get_project_links(page_source):\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    links = soup.find(\"div\", {\"id\": \"results\"}).findAll('a')\n",
    "    project_links = []\n",
    "    logger.info('Getting the project links')\n",
    "    for i in links:\n",
    "        linkspart = 'http://www.cao-ombudsman.org' + i.get('href')\n",
    "        project_links.append(linkspart)\n",
    "    logger.info(\"Found {number_of_projects} links\".format(number_of_projects = len(project_links)))\n",
    "    return project_links\n",
    "\n",
    "##########################################\n",
    "\n",
    "def parse_project_info(page_soup, tag_name):\n",
    "    try:\n",
    "        project_info = page_soup.findAll(tag_name)\n",
    "    except: \n",
    "        project_info = \"\"\n",
    "    \n",
    "    return project_info\n",
    "\n",
    "def parse_project_project_id(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        project, project_id = project_info[1].find(tag_name,{'id': id_name}).get_text().rsplit(' ', 1)\n",
    "    except: \n",
    "        project, project_id = \"\"\n",
    "    \n",
    "    return project, project_id\n",
    "\n",
    "def parse_country(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        country = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except: \n",
    "        country = \"\"\n",
    "    \n",
    "    return country\n",
    "\n",
    "def parse_environmental_category(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        environmental_category = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except:\n",
    "        environmental_category = \"\"\n",
    "    \n",
    "    return environmental_category\n",
    "        \n",
    "def parse_project_type(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        project_type = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except:\n",
    "        project_type = \"\"\n",
    "    \n",
    "    return project_type\n",
    "\n",
    "def parse_sector(tag_name, id_name, project_info):\n",
    "    try: \n",
    "        sector = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except: \n",
    "        sector = \"\"\n",
    "    \n",
    "    return sector\n",
    "\n",
    "def parse_financial_institution(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        financial_institution = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except: \n",
    "        financial_institution = \"\"\n",
    "    \n",
    "    return financial_institution\n",
    "\n",
    "def parse_project_company(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        project_company = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except: \n",
    "        project_company = \"\"\n",
    "    \n",
    "    return project_company\n",
    "\n",
    "def parse_project_loan(tag_name, id_name, project_info):\n",
    "    try: \n",
    "        project_loan = project_info[1].find(tag_name,{'id':id_name}).get_text()\n",
    "    except: \n",
    "        project_loan = \"\"\n",
    "    \n",
    "    return project_loan\n",
    "\n",
    "def parse_date(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        date_filed = project_info[0].find(tag_name,{'id':id_name}).get_text()\n",
    "        year = date_filed[-4:]\n",
    "    except:\n",
    "        date_filed = \"\"\n",
    "        year = \"\"\n",
    "    \n",
    "    return year, date_filed\n",
    "\n",
    "def parse_issues(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        issues = project_info[0].find(tag_name,{'id':id_name}).get_text()\n",
    "    except:\n",
    "        issues = \"\"\n",
    "    \n",
    "    return issues\n",
    "\n",
    "def parse_case_status(tag_name, id_name, project_info):\n",
    "    try:\n",
    "        case_status = project_info[0].find(tag_name,{'id':id_name}).get_text()\n",
    "    except:\n",
    "        case_status = \"\"\n",
    "    \n",
    "    return case_status\n",
    "\n",
    "def parse_project_information(page_soup, url):   \n",
    "\n",
    "    project_info = parse_project_info(page_soup, 'dl') \n",
    "    \n",
    "    case_id = ''.join([i for i in url if i.isdigit()])\n",
    "    \n",
    "    project, project_id = parse_project_project_id('dd', 'ctl00_MainContent_ctrlProjectName', project_info)\n",
    "\n",
    "    country = parse_country('dd', 'ctl00_MainContent_ctrlProjectCountries', project_info)\n",
    "    \n",
    "    environmental_category = parse_environmental_category('dd', 'ctl00_MainContent_ctrlEnvironmentCategory', project_info)\n",
    " \n",
    "    project_type = parse_project_type('dd', 'ctl00_MainContent_ctrlDepartment', project_info)\n",
    "\n",
    "    sector = parse_sector('dd', 'ctl00_MainContent_ctrlSector', project_info)\n",
    "\n",
    "    financial_institution = parse_financial_institution('dd', 'ctl00_MainContent_ctrlInstitution', project_info)\n",
    "        \n",
    "    project_company = parse_project_company('dd', 'ctl00_MainContent_ctrlCompany', project_info)\n",
    "\n",
    "    project_loan = parse_project_loan('dd', 'ctl00_MainContent_ctrlCommitment', project_info)\n",
    "\n",
    "    year, date_filed = parse_date('dd', 'ctl00_MainContent_ctrlDateFilled', project_info)\n",
    "\n",
    "    issues = parse_issues('dd', 'ctl00_MainContent_ctrlConcerns', project_info)\n",
    "\n",
    "    case_status = parse_case_status('dd', 'ctl00_MainContent_ctrlCaseStatus', project_info)\n",
    "    \n",
    "    return url, case_id, project, project_id, country, environmental_category, project_type, sector, \\\n",
    "            financial_institution, project_company, project_loan, year,date_filed ,issues, case_status\n",
    "    \n",
    "\n",
    "def parse_last_completed_stage(tag_name, soup):\n",
    "    # Completed Stages\n",
    "    try:\n",
    "        #completed_stages = soup.findAll(\"span\", {\"class\":\"completed\"})\n",
    "        completed_stages = [i.get_text() for i in soup.findAll(tag_name, {\"class\": \"completed\"})]\n",
    "        last_completed_stage = completed_stages[-1]\n",
    "    except IndexError:\n",
    "        closed_stage = [i.get_text() for i in soup.findAll(tag_name, {\"class\": \"closed\"})]\n",
    "        last_completed_stage = closed_stage\n",
    "    \n",
    "    return last_completed_stage\n",
    "\n",
    "def parse_active_stage(tag_name, soup):\n",
    "    # Active Stage\n",
    "    try: \n",
    "        active_stage = [i.get_text() for i in soup.findAll(tag_name, {\"class\":\"inprocess\"})]\n",
    "        if active_stage == []:\n",
    "            active_stage = None\n",
    "        else:\n",
    "            active_stage = active_stage[0]\n",
    "    except NoSuchElementException:\n",
    "        active_stage = None\n",
    "    \n",
    "    return active_stage\n",
    "\n",
    "def get_eligibility_start_date(active_stage):\n",
    "    # Eligibility Start Date\n",
    "    if active_stage in ('Eligibility: In Process', 'Eligible: In Process'):\n",
    "        eligibility_start_date = 'Completed'\n",
    "    else: \n",
    "        eligibility_start_date = None\n",
    "    \n",
    "    return eligibility_start_date\n",
    "\n",
    "def get_eligibility_end_date(last_completed_stage):\n",
    "    # Eligibility End Date\n",
    "    if last_completed_stage in ('Eligibility: Completed', 'Eligible: Completed'):\n",
    "        eligibility_end_date = 'Completed'\n",
    "    else: \n",
    "        eligibility_end_date = None\n",
    "    \n",
    "    return eligibility_end_date\n",
    "\n",
    "def get_dr_start_date(active_stage, last_completed_stage):\n",
    "    # dr Start Date\n",
    "    if active_stage in ('Assessment Period: In Process', 'Facilitating Settlement: Active'):\n",
    "        dr_start_date = 'Completed'\n",
    "    elif last_completed_stage == 'Assessment Period: Completed':\n",
    "        dr_start_date = 'Completed'\n",
    "    else: \n",
    "        dr_start_date = None \n",
    "    \n",
    "    return dr_start_date\n",
    "\n",
    "def get_dr_end_date(last_completed_stage):\n",
    "    # dr End Date\n",
    "    if last_completed_stage == 'Facilitating Settlement: Completed':\n",
    "        dr_end_date = 'Completed'\n",
    "    else: \n",
    "        dr_end_date = None\n",
    "    \n",
    "    return dr_end_date\n",
    "\n",
    "def get_monitoring_start_date(active_stage):\n",
    "    # Monitoring Start Date\n",
    "    if active_stage in ('Monitoring/Close Out: In Process', 'Monitoring: In Process'):\n",
    "        monitoring_start_date = 'Completed'\n",
    "    else: \n",
    "        monitoring_start_date = None  \n",
    "    \n",
    "    return monitoring_start_date\n",
    "\n",
    "def get_monitoring_end_date(last_completed_stage):\n",
    "    # Monitoring End Date\n",
    "    if last_completed_stage == 'Monitoring/Close Out: Completed':\n",
    "        monitoring_end_date = 'Completed'\n",
    "    else: \n",
    "        monitoring_end_date = None\n",
    "    \n",
    "    return monitoring_end_date\n",
    "\n",
    "def get_cr_start_date(active_stage, last_completed_stage):\n",
    "    # CR Start Date\n",
    "    if active_stage in ('Under Appraisal: In Process', 'Under Audit: In Process'):\n",
    "        cr_start_date = 'Completed'\n",
    "    elif last_completed_stage == 'Under Appraisal: Completed':\n",
    "        cr_start_date = 'Completed'\n",
    "    else: \n",
    "        cr_start_date = None\n",
    "    \n",
    "    return cr_start_date\n",
    "\n",
    "def get_cr_end_date(last_completed_stage):\n",
    "    # CR End Date\n",
    "    if last_completed_stage == 'Under Audit: Compelted':\n",
    "        cr_end_date = 'Completed'\n",
    "    else: \n",
    "        cr_end_date = None\n",
    "        \n",
    "    return cr_end_date\n",
    "\n",
    "def get_date_closed(last_completed_stage):\n",
    "    # Date Closed\n",
    "    if last_completed_stage == 'Monitoring: Completed':\n",
    "        date_closed = 'Completed'\n",
    "    else: \n",
    "        date_closed = None\n",
    "    \n",
    "    return date_closed\n",
    "\n",
    "def parse_project_tracker(soup):\n",
    "\n",
    "    last_completed_stage = parse_last_completed_stage(\"span\", soup)\n",
    " \n",
    "    active_stage = parse_active_stage(\"span\", soup)\n",
    "    \n",
    "    eligibility_start_date = get_eligibility_start_date(active_stage)\n",
    "\n",
    "    eligibility_end_date = get_eligibility_end_date(last_completed_stage)\n",
    "\n",
    "    dr_start_date = get_dr_start_date(active_stage, last_completed_stage)\n",
    "\n",
    "    dr_end_date = get_dr_end_date(last_completed_stage)\n",
    "\n",
    "    monitoring_start_date = get_monitoring_start_date(active_stage)\n",
    "\n",
    "    monitoring_end_date = get_monitoring_end_date(last_completed_stage)\n",
    "\n",
    "    cr_start_date = get_cr_start_date(active_stage, last_completed_stage)\n",
    " \n",
    "    cr_end_date = get_cr_end_date(last_completed_stage)\n",
    "\n",
    "    date_closed = get_date_closed(last_completed_stage)\n",
    "    \n",
    "    return last_completed_stage, active_stage, eligibility_start_date, eligibility_end_date, dr_start_date, dr_end_date, monitoring_start_date, \\\n",
    "           monitoring_end_date, cr_start_date, cr_end_date, date_closed\n",
    "    \n",
    "def process_project_urls(list_of_project_links):\n",
    "    processed_project_dicts = []\n",
    "    # Starting to iterate over the project links and parse the required fields\n",
    "    for l in list_of_project_links:\n",
    "        project_dict = {}\n",
    "        time.sleep(random.choice(range(5,10)))\n",
    "        res = requests.get(l)\n",
    "        if res.status_code != 200:\n",
    "            logger.info(\"%s is broken\".format(l))\n",
    "            continue \n",
    "        else:\n",
    "            #get page data\n",
    "            try:\n",
    "                page_soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            except: \n",
    "                page_soup = \"\"\n",
    "                continue  \n",
    "\n",
    "            # Parsing project information\n",
    "            url, case_id, project, project_id, country, environmental_category, project_type, sector, \\\n",
    "                financial_institution, project_company, project_loan, year, date_filed ,issues, case_status = parse_project_information(page_soup, l)\n",
    "\n",
    "            # Parsing the case tracker\n",
    "            last_completed_stage, active_stage, eligibility_start_date, eligibility_end_date, dr_start_date, dr_end_date, monitoring_start_date, \\\n",
    "               monitoring_end_date, cr_start_date, cr_end_date, date_closed = parse_project_tracker(page_soup)\n",
    "\n",
    "            # Getting the pdf links\n",
    "            document_links = get_doc_links(page_soup)\n",
    "            pdf_links = parse_pdf_links(document_links)\n",
    "\n",
    "            registration_end_date = None\n",
    "            registration_start_date = date_filed\n",
    "\n",
    "\n",
    "            project_dict[Fields.IAM.name] = 'CAO'\n",
    "            project_dict[Fields.IAM_ID.name] = 21\n",
    "            project_dict[Fields.YEAR.name] = year\n",
    "            project_dict[Fields.COUNTRY.name] = country\n",
    "            project_dict[Fields.PROJECT_NAME.name] = project\n",
    "            project_dict[Fields.PROJECT_ID.name] = case_id\n",
    "            project_dict[Fields.PROJECT_NUMBER.name] = project_id\n",
    "            project_dict[Fields.RELATED_PROJECT_NUMBER.name] = None\n",
    "            project_dict[Fields.PROJECT_TYPE.name] = project_type\n",
    "            project_dict[Fields.PROJECT_LOAN_AMOUNT.name] = project_loan\n",
    "            project_dict[Fields.SECTOR.name] = sector\n",
    "            project_dict[Fields.ISSUES.name] = issues\n",
    "            project_dict[Fields.FILERS.name] = None\n",
    "            project_dict[Fields.FILING_DATE.name] = date_filed\n",
    "            project_dict[Fields.ENVIRONMENTAL_CATEGORY.name] = environmental_category\n",
    "            project_dict[Fields.COMPLAINT_STATUS.name] = case_status\n",
    "            project_dict[Fields.REGISTRATION_START_DATE.name] = registration_start_date\n",
    "            project_dict[Fields.REGISTRATION_END_DATE.name] = registration_end_date\n",
    "            project_dict[Fields.ELIGIBILITY_START_DATE.name] = eligibility_start_date\n",
    "            project_dict[Fields.ELIGIBILITY_END_DATE.name] = eligibility_end_date\n",
    "            project_dict[Fields.DISPUTE_RESOLUTION_START_DATE.name] = dr_start_date\n",
    "            project_dict[Fields.DISPUTE_RESOLUTION_END_DATE.name] = dr_end_date\n",
    "            project_dict[Fields.COMPLIANCE_REVIEW_START_DATE.name] = cr_start_date\n",
    "            project_dict[Fields.COMPLIANCE_REVIEW_END_DATE.name] = cr_end_date\n",
    "            project_dict[Fields.MONITORING_START_DATE.name] = monitoring_start_date\n",
    "            project_dict[Fields.MONITORING_END_DATE.name] = monitoring_end_date\n",
    "            project_dict[Fields.IS_COMPLIANCE_REPORT_ISSUED.name] = None\n",
    "            project_dict[Fields.DATE_CLOSED.name] = date_closed\n",
    "            project_dict[Fields.DOCUMENTS.name] = pdf_links\n",
    "            project_dict[Fields.HYPERLINK.name] = l\n",
    "\n",
    "\n",
    "            #writer.writerow(row_data)\n",
    "            processed_project_dicts.append(project_dict)\n",
    "            logger.info(\"Completed project {current_project_number}\".format(current_project_number = list_of_project_links.index(l)))\n",
    "    return processed_project_dicts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    base_url = 'http://www.cao-ombudsman.org/cases/default.aspx'\n",
    "   \n",
    "    s = get_page_source(base_url)\n",
    "    \n",
    "    project_links = get_project_links(s)\n",
    "    \n",
    "    processed_project_dicts = process_project_urls(project_links)\n",
    "\n",
    "    write_csv('cao', processed_project_dicts)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
